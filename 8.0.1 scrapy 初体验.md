# 8.0.1 scrapy 初体验

##### 创建一个Scrapy 项目

```bash
# 通过命令 cd 到想要创建项目的目录中

$ scrapy startproject mingyan  

# 注释：scrapy startproject 为固定格式，可以创建一个mingyan的 Spider项目

```
![image.png](https://cdn.nlark.com/yuque/0/2019/png/235650/1550563706712-1c9feaf0-de91-4609-92be-4ab933d43d59.png#align=left&display=inline&height=157&linkTarget=_blank&name=image.png&originHeight=176&originWidth=768&size=54853&width=684)

创建成功，现在来看一下项目的目录文件：<br />![image.png](https://cdn.nlark.com/yuque/0/2019/png/235650/1550564333528-f38c738e-023e-499d-9d4b-69007f73b617.png#align=left&display=inline&height=199&linkTarget=_blank&name=image.png&originHeight=398&originWidth=564&size=44706&width=282)<br />后面开始爬虫的开发之旅
<<<<<<< HEAD
---
##### 编写一个蜘蛛爬虫(用scrapy.Spider类)
首先在项目文件中创建一个mingyan_spider.py 的Python文件<br />![image.png](https://cdn.nlark.com/yuque/0/2019/png/235650/1550626866511-062ead10-83fe-43cb-ad86-8321b4a704b3.png#align=left&display=inline&height=197&linkTarget=_blank&name=image.png&originHeight=394&originWidth=680&size=55657&width=340)<br />接下来开始梳理下Spider的过程：
* 首先创建一个类，并继承scrapy的一个子类：scrapy.Spider  或者是其他蜘蛛类型，后面会说到，除了Spider还有很多的蜘蛛类型
* 然后定义一个蜘蛛名，name=“”  后面我们运行的话需要用到
* 定义需要爬取的网址，没有网址蜘蛛怎么爬，所以这是必须的
* 继承scrapy的一个方法：start_requests(self)，这个方法的作用就是通过上面定义的链接去爬取页面，简单理解就是下载页面

在 mingyan_spider.py 文件中编写代码

```python
import scrapy

# 需要继承scrapy.Spider 类


class mingyan(scrapy.Spider):

    name = "mingyan_Spider"  # 定义爬虫名字

    def start_requests(self):  # 由此方法通过下面链接爬取页面

        # 定义爬取的链接
        urls = [
            'http://lab.scrapyd.cn/page/1/',
            'http://lab.scrapyd.cn/page/2/',
            'http://lab.scrapyd.cn/page/3/',
        ]

        for url in urls:
            yield scrapy.Request(url=url, callback=self.parse)  # 爬取到的页面如何处理？提交给parse方法处理

    def parse(self, response):
        """
        start_requests已经爬取到页面，那如何提取我们想要的内容呢？那就可以在这个方法里面定义。
        这里的话，并木有定义，只是简单的把页面做了一个保存，并没有涉及提取我们想要的数据，后面会慢慢说到
        也就是用xpath、正则、或是css进行相应提取，这个例子就是让你看看scrapy运行的流程：
        1、定义链接；
        2、通过链接爬取（下载）页面；
        3、定义规则，然后提取数据；
        """

        page = response.url.split("/")[-2]  # 根据上面的链接提取分页,如：/page/1/，提取到的就是：1

        filename = 'mingyan-%s.html' % page  # 拼接文件名，如果是第一页，最终文件名便是：mingyan-1.html

        with open(filename, 'wb') as f:  # python文件操作，不多说了

            f.write(response.body)  # 刚才下载的页面去哪里了？response.body就代表了刚才下载的页面

        self.log('保存文件: %s' % filename)  # 打个日志

```
> 上述编写代码的步骤已经解释的很清楚了，可以通过注释来查看

---
##### 运行爬虫

```bash
scrapy crawl mingyan_Spider
```
> 输入以上命令便可以运行蜘蛛了！这里要重点提醒一下，我们一定要进入：mingyan 这个目录，也就是我们创建的蜘蛛项目目录，以上命令才有效！还有  crawl  后面跟的是你类里面定义的蜘蛛名，也就是：name，并不是项目名、也不是类名，这些细节需要注意下！

然后回到我们的项目目录中可以看到如下文件
![image.png](https://cdn.nlark.com/yuque/0/2019/png/235650/1550629960030-74a62005-d40a-47f5-9147-ab858fec7423.png#align=left&display=inline&height=249&linkTarget=_blank&name=image.png&originHeight=498&originWidth=772&size=55344&width=386)
至此，我们的第一个爬虫程序就写完了，后面还有更复杂、更有意思的爬虫程序。
