# 6.1.3 网络爬虫之Robots协议

Robots协议也称作爬虫协议、机器人协议，它的全名叫作网络爬虫排除标准（Robots Exclusion Protocol），用来告诉爬虫和搜索引擎哪些页面可以抓取，哪些不可以抓取。它通常是一个叫作robots.txt的文本文件，一般放在网站的根目录下。

robots.txt的样例：
```bash
User-agent: *
Disallow: /
Allow: /public/
```
这实现了对所有搜索爬虫只允许爬取public目录的功能，将上述内容保存成robots.txt文件，放在网站的根目录下，和网站的入口文件（比如index.php、index.html和index.jsp等）放在一起。

`Disallow`指定了不允许抓取的目录，比如上例子中设置为/则代表不允许抓取所有页面。


`Allow`一般和`Disallow`一起使用，一般不会单独使用，用来排除某些限制。现在我们设置为`/public/`，则表示所有页面不允许抓取，但可以抓取public目录

---


禁止所有爬虫访问任何目录的代码如下：

```bash
User-agent: * 
Disallow: /
```

```bash
User-agent: *
Disallow:
```

```bash
User-agent: *
Disallow: /private/
Disallow: /tmp/
```

---

只允许某一个爬虫访问的代码如下：

```bash
User-agent: WebCrawler
Disallow:
User-agent: *
Disallow: /
```
这些是robots.txt的一些常见写法。


大家可能会疑惑，爬虫名是哪儿来的？为什么就叫这个名？其实它是有固定名字的了，比如百度的就叫作BaiduSpider。下表列出了一些常见的搜索爬虫的名称及对应的网站。

| 爬虫名称 | 名称 | 网站 |
| :---: | :---: | :---: |
| BaiduSpider | 百度 | www.baidu.com |
| Googlebot | 谷歌 | www.google.com |
| 360Spider | 360搜索 | www.so.com |
| YodaoBot | 有道 | www.youdao.com |
| ia_archiver | Alexa | www.alexa.cn |
| Scooter | altavista | www.altavista.com |


更多说明请到崔大博客：[https://cuiqingcai.com/5511.html](https://cuiqingcai.com/5511.html)
